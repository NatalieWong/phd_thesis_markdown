# Abstract {.unnumbered}

<!-- This is the abstract -->

Nowadays, AR technology has been applied in many innovative projects across the industries to enhance user experience for tasks or products. Taking the advantage of Projection-based AR, screens are no longer needed. Instead, computer-generated graphics are projected onto real-world surfaces and human can directly interact with the virtual graphics.

The current Interactive Information Wall which uses a Kinect Sensor and computer system to detection hand motions could be replaced by the Android Lantern, a portable interactive device which consists of a Raspberry Pi 3 single-board computer, a laser projector and a camera module turning any surface into a fully interactive user interface. Users are able to "click" on the virtually projected graphics using their hands and receive real-time response from the Lantern.

Hand detection machine learning models are trained using TensorFlow Object Detection API to recognize human hands in images captured by the camera module embedded in the Lantern. An Android application is deployed on the Raspberry Pi 3 to process the images, trigger the "click" event and provide the user interface for the Interactive Information Wall.

<!-- ## Summary of chapters -->

<!-- 
For italic, add one * on either side of the text
For bold, add two * on either side of the text
For bold and italic, add _** on either side of the text
-->

\pagenumbering{roman}
\setcounter{page}{1}

\newpage
